<!doctype html>
<!--[if lt IE 7 ]><html itemscope itemtype="http://schema.org/Organization" id="ie6" class="ie ie-old" lang="en-US"><![endif]-->
<!--[if IE 7 ]>   <html itemscope itemtype="http://schema.org/Organization" id="ie7" class="ie ie-old" lang="en-US"><![endif]-->
<!--[if IE 8 ]>   <html itemscope itemtype="http://schema.org/Organization" id="ie8" class="ie ie-old" lang="en-US"><![endif]-->
<!--[if IE 9 ]>   <html itemscope itemtype="http://schema.org/Organization" id="ie9" class="ie" lang="en-US"><![endif]-->
<!--[if gt IE 9]><!--><html itemscope itemtype="http://schema.org/Organization" lang="en-US"><!--<![endif]-->
<head>
    <meta name="author" content="The Apache Software Foundation">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link href="/assets/themes/zeppelin/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="/assets/themes/zeppelin/css/style.css?body=1" rel="stylesheet" type="text/css">
    <link href="/assets/themes/zeppelin/css/syntax.css" rel="stylesheet"  type="text/css" media="screen" /> 

<!--
    <link href="/assets/themes/mahout/css/main.css" rel="stylesheet"  type="text/css" media="screen" /> 
    <link href="/assets/themes/mahout/css/global.css" rel="stylesheet"  type="text/css" media="screen" /> 
    <link href="/assets/themes/mahout/css/global__.css" rel="stylesheet"  type="text/css" media="screen" /> 
-->
    <!-- Le fav and touch icons -->
    <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
    -->

    <!-- Js -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="/assets/themes/zeppelin/bootstrap/js/bootstrap.min.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
  </script>
  <script type="text/javascript">
    var mathjax = document.createElement('script'); 
    mathjax.type = 'text/javascript'; 
    mathjax.async = true;

    mathjax.src = ('https:' == document.location.protocol) ?
        'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' : 
        'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    
      var s = document.getElementsByTagName('script')[0]; 
    s.parentNode.insertBefore(mathjax, s);
  </script>


    <!-- JavaScript 
        <script src="/assets/themes/zeppelin/js/medium.controller.js"></script>
    <script src="/assets/themes/zeppelin/js/moment.min.js"></script>
    <script src="/assets/themes/zeppelin/js/anchor.min.js"></script>
    <script src="/assets/themes/zeppelin/js/docs.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.3.15/angular.min.js"></script>

    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="http://scotch-io.github.io/js/power.js"></script>
    -->


    <!-- atom & rss feed -->
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">


</head>



<body class="">

    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">
            <img src="/assets/themes/mahout/img/mahout-logo.png" width="75" alt="I'm mahout">
            Apache Mahout
          </a>
        </div>
        <nav class="navbar-collapse collapse" role="navigation">
          <ul class="nav navbar-nav navbar-right">

            <!-- Quick Start -->
            <li id="quickstart">
              <a href="/docs/0.13.0/quickstart" >Quick Start</a>
            </li>         

             <li id="samsara">
           <!-- Samsara docs  
             old http://mahout.apache.org/users/sparkbindings/faq.html

           -->
              <a href="#" data-toggle="dropdown" class="dropdown-toggle">Mahout-Samsara<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><span><b>Reference Info</b><span></li>
                <li><a href="/docs/0.13.0/mahout-samsara/incore">In-core Reference</a></li>
                <li><a href="/docs/0.13.0/mahout-samsara/outofcore">Out-of-core Reference</a></li>
                <li><a href="/docs/0.13.0/mahout-samsara/faq">Samsara FAQ</a></li>
                <li role="separator" class="divider"></li>
                <li><span><b>Bindings</b><span></li>
                <li><a href="/docs/0.13.0/mahout-samsara/spark-bindings">Spark Bindings</a></li>
                <li><a href="/docs/0.13.0/mahout-samsara/flink-bindings">Flink Bindings</a></li>
                <li><a href="/docs/0.13.0/mahout-samsara/flink-bindings">H20 Bindings</a></li>
              </ul>
            </li>

             <li id="tutorials">
           <!-- Tutorials -->
              <a href="#" data-toggle="dropdown" class="dropdown-toggle">Tutorials<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><span><b>Spark Examples using Samsara</b><span></li>
                <li><a href="/docs/0.13.0/tutorials/samsara-spark-shell">Samsara in Spark Shell</a></li>
                <li><a href="/docs/0.13.0/tutorials/build-app">Samsara Spark Application</a></li>
                <li><a href="/docs/0.13.0/tutorials/text-classification">Text Classification</a></li>
                <li role="separator" class="divider"></li>
                <li><span><b>Fl</b><span></li>
                <li><a href="/docs/0.13.1-SNAPSHOT">tbd1</a></li>
                <li><a href="/docs/0.13.1-SNAPSHOT">tbd2</a></li>
                <li role="separator" class="divider"></li>
                <li><span><b>MapReduce</b><span></li>
                <li><a href="/docs/0.13.1-SNAPSHOT">tbd1</a></li>
                <li><a href="/docs/0.13.1-SNAPSHOT">tbd2</a></li>
              </ul>
            </li>


            <!-- Algorithms (Samsara / MR) -->
            <li id="algorithms">
              <a href="#" data-toggle="dropdown" class="dropdown-toggle">Algorithms<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li><span><b>Samsara Code</b><span></li>
                <li><a href="/docs/0.13.0/algorithms/samsara/dssvd">Dist SVD</a></li>
                <li><a href="/docs/0.13.0/algorithms/samsara/dspca">Dist PCA</a></li>
                <li><a href="/docs/0.13.0/algorithms/samsara/dqr">Dist QR</a></li>
                <li><a href="/docs/0.13.0/algorithms/samsara/dals">Dist ALS</a></li>
                <li role="separator" class="divider"></li>
                <li><span><b>Command Line</b><span></li>
                <li><a href="/docs/0.13.0">algo1</a></li>
                <li><a href="/docs/0.13.0">algo2</a></li>
                <li role="separator" class="divider"></li>
                <li><span><b>xxxx</b>&nbsp;(xxxxx)<span></li>
                <li><a href="/docs/0.13.1-SNAPSHOT">xxxxx</a></li>
              </ul>
            </li>

            <!-- Scala Docs -->
            <li id="scaladocs">
              <a href="tbd" data-toggle="dropdown" class="dropdown-toggle">Scala Docs<b class="caret"></b></a>
              <ul class="dropdown-menu">
                <li class="title"><span><b>TODO - Needs update per new scaladocs</b><span></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout Math</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math-scala/index.html">Mahout Math Scala bindings</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.htmlhttp://apache.github.io/mahout/0.10.1/docs/mahout-spark/index.html">Mahout Spark bindings</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout Spark bindings shell</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout H2O backend Scaladoc</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout H2O backend Javadoc</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout HDFS</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout Map-Reduce</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout Examples</a></li>
                <li><a href="http://apache.github.io/mahout/0.10.1/docs/mahout-math/index.html">Mahout Integration</a></li>
              </ul>
            </li>



















        </ul>
        </nav><!--/.navbar-collapse -->
      </div>
    </div>

        <article>
        <div class="container">
            <div class="row">
                <div class="col-md-10 col-md-offset-1">
                    <p>#Building a text classifier in Mahout’s Spark Shell</p>

<p>This tutorial will take you through the steps used to train a Multinomial Naive Bayes model and create a text classifier based on that model using the <code>mahout spark-shell</code>.</p>

<h2 id="prerequisites">Prerequisites</h2>
<p>This tutorial assumes that you have your Spark environment variables set for the <code>mahout spark-shell</code> see: <a href="http://mahout.apache.org/users/sparkbindings/play-with-shell.html">Playing with Mahout’s Shell</a>.  As well we assume that Mahout is running in cluster mode (i.e. with the <code>MAHOUT_LOCAL</code> environment variable <strong>unset</strong>) as we’ll be reading and writing to HDFS.</p>

<h2 id="downloading-and-vectorizing-the-wikipedia-dataset">Downloading and Vectorizing the Wikipedia dataset</h2>
<p><em>As of Mahout v. 0.10.0, we are still reliant on the MapReduce versions of <code>mahout seqwiki</code> and <code>mahout seq2sparse</code> to extract and vectorize our text.  A</em> <a href="https://issues.apache.org/jira/browse/MAHOUT-1663"><em>Spark implementation of seq2sparse</em></a> <em>is in the works for Mahout v. 0.11.</em> However, to download the Wikipedia dataset, extract the bodies of the documentation, label each document and vectorize the text into TF-IDF vectors, we can simpmly run the <a href="https://github.com/apache/mahout/blob/master/examples/bin/classify-wikipedia.sh">wikipedia-classifier.sh</a> example.</p>

<pre><code>Please select a number to choose the corresponding task to run
1. CBayes (may require increased heap space on yarn)
2. BinaryCBayes
3. clean -- cleans up the work area in /tmp/mahout-work-wiki
Enter your choice :
</code></pre>

<p>Enter (2). This will download a large recent XML dump of the Wikipedia database, into a <code>/tmp/mahout-work-wiki</code> directory, unzip it and  place it into HDFS.  It will run a <a href="http://mahout.apache.org/users/classification/wikipedia-classifier-example.html">MapReduce job to parse the wikipedia set</a>, extracting and labeling only pages with category tags for [United States] and [United Kingdom] (~11600 documents). It will then run <code>mahout seq2sparse</code> to convert the documents into TF-IDF vectors.  The script will also a build and test a <a href="http://mahout.apache.org/users/classification/bayesian.html">Naive Bayes model using MapReduce</a>.  When it is completed, you should see a confusion matrix on your screen.  For this tutorial, we will ignore the MapReduce model, and build a new model using Spark based on the vectorized text output by <code>seq2sparse</code>.</p>

<h2 id="getting-started">Getting Started</h2>

<p>Launch the <code>mahout spark-shell</code>.  There is an example script: <code>spark-document-classifier.mscala</code> (.mscala denotes a Mahout-Scala script which can be run similarly to an R script).   We will be walking through this script for this tutorial but if you wanted to simply run the script, you could just issue the command:</p>

<pre><code>mahout&gt; :load /path/to/mahout/examples/bin/spark-document-classifier.mscala
</code></pre>

<p>For now, lets take the script apart piece by piece.  You can cut and paste the following code blocks into the <code>mahout spark-shell</code>.</p>

<h2 id="imports">Imports</h2>

<p>Our Mahout Naive Bayes imports:</p>

<pre><code>import org.apache.mahout.classifier.naivebayes._
import org.apache.mahout.classifier.stats._
import org.apache.mahout.nlp.tfidf._
</code></pre>

<p>Hadoop imports needed to read our dictionary:</p>

<pre><code>import org.apache.hadoop.io.Text
import org.apache.hadoop.io.IntWritable
import org.apache.hadoop.io.LongWritable
</code></pre>

<h2 id="read-in-our-full-set-from-hdfs-as-vectorized-by-seq2sparse-in-classify-wikipediash">Read in our full set from HDFS as vectorized by seq2sparse in classify-wikipedia.sh</h2>

<pre><code>val pathToData = "/tmp/mahout-work-wiki/"
val fullData = drmDfsRead(pathToData + "wikipediaVecs/tfidf-vectors")
</code></pre>

<h2 id="extract-the-category-of-each-observation-and-aggregate-those-observations-by-category">Extract the category of each observation and aggregate those observations by category</h2>

<pre><code>val (labelIndex, aggregatedObservations) = SparkNaiveBayes.extractLabelsAndAggregateObservations(
                                                             fullData)
</code></pre>

<h2 id="build-a-muitinomial-naive-bayes-model-and-self-test-on-the-training-set">Build a Muitinomial Naive Bayes model and self test on the training set</h2>

<pre><code>val model = SparkNaiveBayes.train(aggregatedObservations, labelIndex, false)
val resAnalyzer = SparkNaiveBayes.test(model, fullData, false)
println(resAnalyzer)
</code></pre>

<p>printing the <code>ResultAnalyzer</code> will display the confusion matrix.</p>

<h2 id="read-in-the-dictionary-and-document-frequency-count-from-hdfs">Read in the dictionary and document frequency count from HDFS</h2>

<pre><code>val dictionary = sdc.sequenceFile(pathToData + "wikipediaVecs/dictionary.file-0",
                                  classOf[Text],
                                  classOf[IntWritable])
val documentFrequencyCount = sdc.sequenceFile(pathToData + "wikipediaVecs/df-count",
                                              classOf[IntWritable],
                                              classOf[LongWritable])

// setup the dictionary and document frequency count as maps
val dictionaryRDD = dictionary.map { 
                                case (wKey, wVal) =&gt; wKey.asInstanceOf[Text]
                                                         .toString() -&gt; wVal.get() 
                                   }
                                   
val documentFrequencyCountRDD = documentFrequencyCount.map {
                                        case (wKey, wVal) =&gt; wKey.asInstanceOf[IntWritable]
                                                                 .get() -&gt; wVal.get() 
                                                           }

val dictionaryMap = dictionaryRDD.collect.map(x =&gt; x._1.toString -&gt; x._2.toInt).toMap
val dfCountMap = documentFrequencyCountRDD.collect.map(x =&gt; x._1.toInt -&gt; x._2.toLong).toMap
</code></pre>

<h2 id="define-a-function-to-tokenize-and-vectorize-new-text-using-our-current-dictionary">Define a function to tokenize and vectorize new text using our current dictionary</h2>

<p>For this simple example, our function <code>vectorizeDocument(...)</code> will tokenize a new document into unigrams using native Java String methods and vectorize using our dictionary and document frequencies. You could also use a <a href="https://lucene.apache.org/core/">Lucene</a> analyzer for bigrams, trigrams, etc., and integrate Apache <a href="https://tika.apache.org/">Tika</a> to extract text from different document types (PDF, PPT, XLS, etc.).  Here, however we will keep it simple, stripping and tokenizing our text using regexs and native String methods.</p>

<pre><code>def vectorizeDocument(document: String,
                        dictionaryMap: Map[String,Int],
                        dfMap: Map[Int,Long]): Vector = {
    val wordCounts = document.replaceAll("[^\\p{L}\\p{Nd}]+", " ")
                                .toLowerCase
                                .split(" ")
                                .groupBy(identity)
                                .mapValues(_.length)         
    val vec = new RandomAccessSparseVector(dictionaryMap.size)
    val totalDFSize = dfMap(-1)
    val docSize = wordCounts.size
    for (word &lt;- wordCounts) {
        val term = word._1
        if (dictionaryMap.contains(term)) {
            val tfidf: TermWeight = new TFIDF()
            val termFreq = word._2
            val dictIndex = dictionaryMap(term)
            val docFreq = dfCountMap(dictIndex)
            val currentTfIdf = tfidf.calculate(termFreq,
                                               docFreq.toInt,
                                               docSize,
                                               totalDFSize.toInt)
            vec.setQuick(dictIndex, currentTfIdf)
        }
    }
    vec
}
</code></pre>

<h2 id="setup-our-classifier">Setup our classifier</h2>

<pre><code>val labelMap = model.labelIndex
val numLabels = model.numLabels
val reverseLabelMap = labelMap.map(x =&gt; x._2 -&gt; x._1)

// instantiate the correct type of classifier
val classifier = model.isComplementary match {
    case true =&gt; new ComplementaryNBClassifier(model)
    case _ =&gt; new StandardNBClassifier(model)
}
</code></pre>

<h2 id="define-an-argmax-function">Define an argmax function</h2>

<p>The label with the highest score wins the classification for a given document.</p>

<pre><code>def argmax(v: Vector): (Int, Double) = {
    var bestIdx: Int = Integer.MIN_VALUE
    var bestScore: Double = Integer.MIN_VALUE.asInstanceOf[Int].toDouble
    for(i &lt;- 0 until v.size) {
        if(v(i) &gt; bestScore){
            bestScore = v(i)
            bestIdx = i
        }
    }
    (bestIdx, bestScore)
}
</code></pre>

<h2 id="define-our-tf-idf-vector-classifier">Define our TF(-IDF) vector classifier</h2>

<pre><code>def classifyDocument(clvec: Vector) : String = {
    val cvec = classifier.classifyFull(clvec)
    val (bestIdx, bestScore) = argmax(cvec)
    reverseLabelMap(bestIdx)
}
</code></pre>

<h2 id="two-sample-news-articles-united-states-football-and-united-kingdom-football">Two sample news articles: United States Football and United Kingdom Football</h2>

<pre><code>// A random United States football article
// http://www.reuters.com/article/2015/01/28/us-nfl-superbowl-security-idUSKBN0L12JR20150128
val UStextToClassify = new String("(Reuters) - Super Bowl security officials acknowledge" +
    " the NFL championship game represents a high profile target on a world stage but are" +
    " unaware of any specific credible threats against Sunday's showcase. In advance of" +
    " one of the world's biggest single day sporting events, Homeland Security Secretary" +
    " Jeh Johnson was in Glendale on Wednesday to review security preparations and tour" +
    " University of Phoenix Stadium where the Seattle Seahawks and New England Patriots" +
    " will battle. Deadly shootings in Paris and arrest of suspects in Belgium, Greece and" +
    " Germany heightened fears of more attacks around the world and social media accounts" +
    " linked to Middle East militant groups have carried a number of threats to attack" +
    " high-profile U.S. events. There is no specific credible threat, said Johnson, who" + 
    " has appointed a federal coordination team to work with local, state and federal" +
    " agencies to ensure safety of fans, players and other workers associated with the" + 
    " Super Bowl. I'm confident we will have a safe and secure and successful event." +
    " Sunday's game has been given a Special Event Assessment Rating (SEAR) 1 rating, the" +
    " same as in previous years, except for the year after the Sept. 11, 2001 attacks, when" +
    " a higher level was declared. But security will be tight and visible around Super" +
    " Bowl-related events as well as during the game itself. All fans will pass through" +
    " metal detectors and pat downs. Over 4,000 private security personnel will be deployed" +
    " and the almost 3,000 member Phoenix police force will be on Super Bowl duty. Nuclear" +
    " device sniffing teams will be deployed and a network of Bio-Watch detectors will be" +
    " set up to provide a warning in the event of a biological attack. The Department of" +
    " Homeland Security (DHS) said in a press release it had held special cyber-security" +
    " and anti-sniper training sessions. A U.S. official said the Transportation Security" +
    " Administration, which is responsible for screening airline passengers, will add" +
    " screeners and checkpoint lanes at airports. Federal air marshals, behavior detection" +
    " officers and dog teams will help to secure transportation systems in the area. We" +
    " will be ramping it (security) up on Sunday, there is no doubt about that, said Federal"+
    " Coordinator Matthew Allen, the DHS point of contact for planning and support. I have" +
    " every confidence the public safety agencies that represented in the planning process" +
    " are going to have their best and brightest out there this weekend and we will have" +
    " a very safe Super Bowl.")

// A random United Kingdom football article
// http://www.reuters.com/article/2015/01/26/manchester-united-swissquote-idUSL6N0V52RZ20150126
val UKtextToClassify = new String("(Reuters) - Manchester United have signed a sponsorship" +
    " deal with online financial trading company Swissquote, expanding the commercial" +
    " partnerships that have helped to make the English club one of the richest teams in" +
    " world soccer. United did not give a value for the deal, the club's first in the sector," +
    " but said on Monday it was a multi-year agreement. The Premier League club, 20 times" +
    " English champions, claim to have 659 million followers around the globe, making the" +
    " United name attractive to major brands like Chevrolet cars and sportswear group Adidas." +
    " Swissquote said the global deal would allow it to use United's popularity in Asia to" +
    " help it meet its targets for expansion in China. Among benefits from the deal," +
    " Swissquote's clients will have a chance to meet United players and get behind the scenes" +
    " at the Old Trafford stadium. Swissquote is a Geneva-based online trading company that" +
    " allows retail investors to buy and sell foreign exchange, equities, bonds and other asset" +
    " classes. Like other retail FX brokers, Swissquote was left nursing losses on the Swiss" +
    " franc after Switzerland's central bank stunned markets this month by abandoning its cap" +
    " on the currency. The fallout from the abrupt move put rival and West Ham United shirt" +
    " sponsor Alpari UK into administration. Swissquote itself was forced to book a 25 million" +
    " Swiss francs ($28 million) provision for its clients who were left out of pocket" +
    " following the franc's surge. United's ability to grow revenues off the pitch has made" +
    " them the second richest club in the world behind Spain's Real Madrid, despite a" +
    " downturn in their playing fortunes. United Managing Director Richard Arnold said" +
    " there was still lots of scope for United to develop sponsorships in other areas of" +
    " business. The last quoted statistics that we had showed that of the top 25 sponsorship" +
    " categories, we were only active in 15 of those, Arnold told Reuters. I think there is a" +
    " huge potential still for the club, and the other thing we have seen is there is very" +
    " significant growth even within categories. United have endured a tricky transition" +
    " following the retirement of manager Alex Ferguson in 2013, finishing seventh in the" +
    " Premier League last season and missing out on a place in the lucrative Champions League." +
    " ($1 = 0.8910 Swiss francs) (Writing by Neil Maidment, additional reporting by Jemima" + 
    " Kelly; editing by Keith Weir)")
</code></pre>

<h2 id="vectorize-and-classify-our-documents">Vectorize and classify our documents</h2>

<pre><code>val usVec = vectorizeDocument(UStextToClassify, dictionaryMap, dfCountMap)
val ukVec = vectorizeDocument(UKtextToClassify, dictionaryMap, dfCountMap)

println("Classifying the news article about superbowl security (united states)")
classifyDocument(usVec)

println("Classifying the news article about Manchester United (united kingdom)")
classifyDocument(ukVec)
</code></pre>

<h2 id="tie-everything-together-in-a-new-method-to-classify-text">Tie everything together in a new method to classify text</h2>

<pre><code>def classifyText(txt: String): String = {
    val v = vectorizeDocument(txt, dictionaryMap, dfCountMap)
    classifyDocument(v)
}
</code></pre>

<h2 id="now-we-can-simply-call-our-classifytext-method-on-any-string">Now we can simply call our classifyText(…) method on any String</h2>

<pre><code>classifyText("Hello world from Queens")
classifyText("Hello world from London")
</code></pre>

<h2 id="model-persistance">Model persistance</h2>

<p>You can save the model to HDFS:</p>

<pre><code>model.dfsWrite("/path/to/model")
</code></pre>

<p>And retrieve it with:</p>

<pre><code>val model =  NBModel.dfsRead("/path/to/model")
</code></pre>

<p>The trained model can now be embedded in an external application.</p>

                </div>
            </div>
        </div>
    </article>


    <footer class="site-footer">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <p class="text-muted">This text is footer text. Footer text goes here.</p>
                </div>
            </div>
        </div>
    </footer>

    </body>
</html>
